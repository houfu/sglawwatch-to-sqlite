name: Fetch

on:
  schedule:
    # Run at 8:00 AM UTC (4:00 PM Singapore time) every day
    - cron: '0 3 * * *'
  workflow_dispatch:  # Allow manual triggering as well

jobs:
  fetch-and-upload:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install boto3
          pip install awscli

      - name: Fetch headlines and upload to Contabo Object Storage
        env:
          JINA_API_TOKEN: ${{ secrets.JINA_API_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
          AWS_REQUEST_CHECKSUM_CALCULATION: when_required
          AWS_RESPONSE_CHECKSUM_VALIDATION: when_required
        run: |
          # Create date-based path
          DATE_PATH=$(date +"%Y/%m/%d")
          S3_PATH="s3://$S3_BUCKET_NAME/sglawwatch/$DATE_PATH/"

          echo "Fetching headlines and storing to $S3_PATH"
          # Use S3_ENDPOINT_URL environment variable for Contabo endpoint
          sglawwatch-to-sqlite fetch all "$S3_PATH"

          # Copy to latest path at top level using AWS CLI
          echo "Copying to latest path"
          aws s3 cp "$S3_PATH/sglawwatch.db" "s3://$S3_BUCKET_NAME/latest/sglawwatch.db" \
          --endpoint-url "$S3_ENDPOINT_URL" \
          --region default

          echo "Successfully fetched headlines and uploaded to Object Storage"
          
