<documents>
<document index="1">
<source>./LICENSE</source>
<document_content>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

</document_content>
</document>
<document index="2">
<source>./README.md</source>
<document_content>
# sglawwatch-to-sqlite

[![Changelog](https://img.shields.io/github/v/release/houfu/sglawwatch-to-sqlite?include_prereleases&label=changelog)](https://github.com/houfu/sglawwatch-to-sqlite/releases)
[![Tests](https://github.com/houfu/sglawwatch-to-sqlite/actions/workflows/test.yml/badge.svg)](https://github.com/houfu/sglawwatch-to-sqlite/actions/workflows/test.yml)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/houfu/sglawwatch-to-sqlite/blob/master/LICENSE)

A tool to track Singapore's legal developments by importing Singapore Law Watch's RSS feed into a searchable SQLite database.

## Features

- Fetch and parse legal headlines from Singapore Law Watch's RSS feed
- Store headlines with metadata, content, and AI-generated summaries in SQLite
- Full-text search capabilities for legal research
- Support for both local and S3 storage
- Automatic handling of duplicate entries
- Skip advertisements in the feed
- Command-line interface for easy integration into workflows

## Installation

**Requirements:**
- Python 3.9 or higher

Clone this repository from GitHub:

```bash
git clone https://github.com/houfu/sglawwatch-to-sqlite.git
cd sglawwatch-to-sqlite
```

Install the package and its dependencies using `uv`:

```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

## Usage

### Basic Usage

Fetch recent headlines and store them in a local SQLite database:

```bash
sglawwatch-to-sqlite fetch headlines
```

This will create a `sglawwatch.db` file in the current directory.

### Specifying Storage Location

You can specify a different directory for the database:

```bash
sglawwatch-to-sqlite fetch headlines ./data
```

### Using S3 Storage

Store the database in an S3 bucket:

```bash
sglawwatch-to-sqlite fetch headlines s3://my-bucket/path/
```

Or use the `S3_BUCKET_NAME` environment variable:

```bash
export S3_BUCKET_NAME=my-bucket
sglawwatch-to-sqlite fetch headlines s3:///path/
```

### Fetch All Entries

To fetch all entries regardless of what was previously fetched:

```bash
sglawwatch-to-sqlite fetch headlines --all
```

### Fetch All Available Feeds

To fetch all available feeds (currently only headlines):

```bash
sglawwatch-to-sqlite fetch all
```

To reset and fetch all entries from scratch:

```bash
sglawwatch-to-sqlite fetch all --reset
```

### Help

For help on available commands:

```bash
sglawwatch-to-sqlite --help
```

For help on specific commands:

```bash
sglawwatch-to-sqlite fetch headlines --help
```

## Environment Variables

- `JINA_API_TOKEN`: API token for Jina AI (used to extract article content)
- `OPENAI_API_KEY`: API key for OpenAI (used to generate summaries)
- `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`: AWS credentials for S3 storage
- `S3_BUCKET_NAME`: Default S3 bucket name (optional, can be specified in URI)

## Database Schema

The tool creates the following tables:

- `headlines`: Stores article headlines with metadata, content, and summaries
- `metadata`: Stores the last updated timestamp
- `schema_versions`: Tracks database schema versions

The headlines table includes full-text search capabilities on title and summary fields.

```sql
-- Search for headlines containing specific keywords
SELECT title, date, author, summary 
FROM headlines_fts 
WHERE headlines_fts MATCH 'digital trade' 
ORDER BY date DESC;

-- Search with multiple terms across both title and summary
SELECT title, date, author, summary 
FROM headlines_fts 
WHERE headlines_fts MATCH 'constitution rights privacy' 
ORDER BY rank;
```

## Using with Datasette
For an interactive web interface to explore your legal headlines database, Datasette is an excellent companion tool:
```bash
# Install Datasette
pip install datasette

# Start Datasette with your database
datasette sglawwatch.db

# Or publish to a Datasette hosting service
datasette publish vercel sglawwatch.db --project=singapore-law-watch
```
Datasette provides a user-friendly interface for browsing tables, running custom queries, and even sharing your legal database with colleagues.

## Development

To contribute to this tool, first checkout the code. Then create a new virtual environment:

```bash
cd sglawwatch-to-sqlite
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install the dependencies and test dependencies:

```bash
pip install -e '.[test]'
```

To run the tests:

```bash
python -m pytest
```

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](https://github.com/houfu/sglawwatch-to-sqlite/blob/master/LICENSE) file for details.
</document_content>
</document>
<document index="3">
<source>./pyproject.toml</source>
<document_content>
[project]
name = "sglawwatch-to-sqlite"
version = "0.2.0"
description = "Track Singapore's legal developments by importing Singapore Law Watch's RSS feed into a searchable SQLite database"
readme = "README.md"
requires-python = ">=3.9"
classifiers = []
dependencies = [ "click", "feedparser>=6.0.11", "httpx>=0.28.1", "openai>=1.78.0", "sqlite-utils>=3.38", "tenacity>=9.0.0", "boto3>=1.37.38",]
[[project.authors]]
name = "Ang Hou Fu"

[build-system]
requires = [ "setuptools",]
build-backend = "setuptools.build_meta"

[project.license]
file = "LICENSE"

[project.urls]
Homepage = "https://github.com/houfu/sglawwatch-to-sqlite"
Changelog = "https://github.com/houfu/sglawwatch-to-sqlite/releases"
Issues = "https://github.com/houfu/sglawwatch-to-sqlite/issues"
CI = "https://github.com/houfu/sglawwatch-to-sqlite/actions"

[project.scripts]
sglawwatch-to-sqlite = "sglawwatch_to_sqlite.cli:cli"

[project.optional-dependencies]
test = [ "pytest",]

[tool.uv]
dev-dependencies = [ "pytest-asyncio>=0.24.0", "pytest>=8.3.5",]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"

</document_content>
</document>
<document index="4">
<source>./tests/conftest.py</source>
<document_content>
import asyncio
import os
from unittest.mock import MagicMock, AsyncMock

import feedparser
import pytest
import sqlite_utils


@pytest.fixture
def mock_db():
    """Fixture providing a mock SQLite database."""
    mock = MagicMock(spec=sqlite_utils.Database)

    # Set up mock tables
    headlines_table = MagicMock()
    metadata_table = MagicMock()

    # Configure __getitem__ to return appropriate tables
    mock.__getitem__.side_effect = lambda table_name: {
        'headlines': headlines_table,
        'metadata': metadata_table,
    }.get(table_name, MagicMock())

    return mock


@pytest.fixture
def mock_metadata_table(mock_db):
    """Fixture for the metadata table with controlled behavior."""
    metadata = mock_db['metadata']

    # Mock the get method to return a value or raise NotFoundError
    def get_side_effect(key):
        if key == "headlines_last_updated":
            return {"key": key, "value": "2025-05-01T00:00:00"}
        else:
            from sqlite_utils.db import NotFoundError
            raise NotFoundError(f"No such item: {key}")

    metadata.get.side_effect = get_side_effect
    return metadata


@pytest.fixture
def mock_feed_entries():
    """Fixture providing sample RSS feed entries in XML format."""
    return """
 <rss version="2.0">
<channel>
<title>Singapore Law Watch - SLW Today</title>
<link>http://www.singaporelawwatch.sg</link>
<copyright>Copyright 2018, Singapore Government, Singapore Academy of Law.</copyright>
<generator>Singapore Law Watch</generator>
<language>en-gb</language>
<item>
<title>Law don argues in inmatesâ€™ appeal that parts of Misuse of Drugs Act are unconstitutional</title>
<link>https://www.singaporelawwatch.sg/Headlines/Law-don-argues-in-inmates-appeal-that-parts-of-Misuse-of-Drugs-Act-are-unconstitutional</link>
<description><p>Deputy A-G counters that presumption of innocence not provided for in Constitution.</p></description>
<author>Straits Times: Selina Lum</author>
<category>Straits Times</category>
<pubDate>08 May 2025 00:01:00</pubDate>
<eventDate/>
</item>
<item>
<title>Singapore and EU sign digital trade pact, deepening cooperation amid global uncertainties</title>
<link>https://www.singaporelawwatch.sg/Headlines/Singapore-and-EU-sign-digital-trade-pact-deepening-cooperation-amid-global-uncertainties</link>
<description><p>The agreement supplements the EU-Singapore Free Trade Agreement (EUSFTA) that entered into force in 2019.</p></description>
<author>Straits Times: Angela Tan</author>
<category>Straits Times</category>
<pubDate>08 May 2025 00:01:00</pubDate>
<eventDate/>
</item>
<item>
<title>Generative AI a top priority for firms but privacy concerns remain: GIC survey</title>
<link>https://www.singaporelawwatch.sg/Headlines/Generative-AI-a-top-priority-for-firms-but-privacy-concerns-remain-GIC-survey</link>
<description><p>Many exploring its use for software development and IT applications.</p></description>
<author>Straits Times: Timothy Goh</author>
<category>Straits Times</category>
<pubDate>08 May 2025 00:01:00</pubDate>
<eventDate/>
</item>
<item>
<title>ADV: Law & Technology in Singapore book launch - 23 May</title>
<link>https://store.lawnet.com/law-and-technology-in-singapore-navigating-the-future-of-legal-practice-in-a-digital-age.html?utm_source=slw_edm&utm_medium=slwleaderboard&utm_campaign=2025may-poem_lawntech2esem-slw_edm-slwleaderboard-&utm_id=poem_lawntech2esem</link>
<description><p>This <a href="https://store.lawnet.com/seminar-book-law-and-technology-in-singapore-navigating-the-future-of-legal-practice-in-a-digital-age.html?utm_source=slw_edm&amp;utm_medium=slwleaderboard&amp;utm_campaign=2025may-poem_lawntech2esem-slw_edm-slwleaderboard-&amp;utm_id=poem_lawntech2esem" target="_blank"><span style="color:#0033ff;">seminar</span></a> marks the release of the second edition of Law and Technology in Singapore, offering a timely exploration of how emerging technologies are reshaping legal practice and frameworks in Singapore. The authors will provide an overview of how technology intersects with various areas of Singapore law, examine current legal practices, and explore future developments.</p></description>
<author>Academy Publishing</author>
<category>Academy Publishing</category>
<pubDate>08 May 2025 00:01:00</pubDate>
<eventDate/>
</item>
</channel>   
</rss>
    """


@pytest.fixture
def mock_feed():
    """Fixture providing a mock feedparser result."""
    mock = MagicMock(spec=feedparser.FeedParserDict)
    mock.bozo = False

    # Create a list of entry dictionaries instead of using XML string
    mock.entries = [
        {
            'title': 'Law don argues in inmates\' appeal that parts of Misuse of Drugs Act are unconstitutional',
            'link': 'https://www.singaporelawwatch.sg/Headlines/Law-don-argues-in-inmates-appeal-that-parts-of-Misuse-of-Drugs-Act-are-unconstitutional',
            'description': '<p>Deputy A-G counters that presumption of innocence not provided for in Constitution.</p>',
            'author': 'Straits Times: Selina Lum',
            'category': 'Straits Times',
            'published': '08 May 2025 00:01:00',
        },
        {
            'title': 'Singapore and EU sign digital trade pact, deepening cooperation amid global uncertainties',
            'link': 'https://www.singaporelawwatch.sg/Headlines/Singapore-and-EU-sign-digital-trade-pact-deepening-cooperation-amid-global-uncertainties',
            'description': '<p>The agreement supplements the EU-Singapore Free Trade Agreement (EUSFTA) that entered into force in 2019.</p>',
            'author': 'Straits Times: Angela Tan',
            'category': 'Straits Times',
            'published': '08 May 2025 00:01:00',
        },
        {
            'title': 'ADV: Law & Technology in Singapore book launch - 23 May',
            'link': 'https://store.lawnet.com/law-and-technology-in-singapore-navigating-the-future-of-legal-practice-in-a-digital-age.html',
            'description': '<p>This seminar marks the release of the second edition...</p>',
            'author': 'Academy Publishing',
            'category': 'Academy Publishing',
            'published': '08 May 2025 00:01:00',
        }
    ]

    mock.feed = {
        'title': 'Singapore Law Watch Headlines',
        'updated': 'Fri, 09 May 2025 12:00:00 GMT'
    }
    return mock


@pytest.fixture
def mock_feed_error():
    """Fixture providing a mock feedparser result with an error."""
    mock = MagicMock(spec=feedparser.FeedParserDict)
    mock.bozo = True
    mock.bozo_exception = "XML parsing error: mismatched tag at line 42"
    mock.entries = []
    mock.feed = {}
    return mock


@pytest.fixture
def mock_jina_response():
    """Fixture providing a mock response from Jina API."""
    return """
    <article>
        <h1>New Court Decision on Property Law</h1>
        <p>In a landmark decision handed down yesterday, the High Court ruled 
        that property owners must adhere to stricter disclosure requirements 
        when selling residential properties.</p>
        <p>The decision in Smith v. Jones (2025) will have far-reaching implications
        for the real estate market in Singapore.</p>
        <p>Legal experts suggest this may lead to significant changes in how 
        property transactions are conducted nationwide.</p>
    </article>
    """


@pytest.fixture
def mock_openai_response():
    """Fixture providing a mock response from OpenAI API."""
    return "The High Court has established stricter disclosure requirements for property sellers in a landmark ruling. Smith v. Jones (2025) mandates comprehensive disclosure of property defects, potentially transforming Singapore's real estate transactions and strengthening buyer protections. Industry experts anticipate significant adjustments to current practices."


@pytest.fixture
def mock_env_vars():
    """Fixture to set and restore environment variables."""
    original_env = os.environ.copy()
    os.environ['JINA_API_TOKEN'] = 'mock-jina-token'
    os.environ['OPENAI_API_KEY'] = 'mock-openai-key'

    yield

    # Reset environment to original state
    os.environ.clear()
    os.environ.update(original_env)


@pytest.fixture
def mock_httpx_client():
    """Fixture providing a mock httpx.AsyncClient."""
    mock_client = AsyncMock()

    # Mock response for Jina API
    mock_response = AsyncMock()
    mock_response.text = "<article>Sample article content</article>"
    mock_response.status_code = 200

    # Configure the client to return the mock response
    mock_client.__aenter__.return_value.get.return_value = mock_response

    return mock_client


@pytest.fixture
def mock_openai_client():
    """Fixture providing a mock OpenAI client."""
    mock_client = AsyncMock()

    # Create a mock response object
    mock_response = AsyncMock()
    mock_response.output_text = "This is a concise legal summary."

    # Set up the client to return the mock response
    mock_client.responses.create.return_value = mock_response

    return mock_client


@pytest.fixture
def event_loop():
    """Create an instance of the default event loop for each test case.

    This is needed for pytest-asyncio to work properly.
    """
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
def mock_boto3_client():
    """Mock boto3 client for S3 testing."""
    mock_client = MagicMock()

    # Configure the download_file method
    mock_client.download_file = MagicMock()

    # Configure the upload_file method
    mock_client.upload_file = MagicMock()

    return mock_client


@pytest.fixture
def mock_boto3():
    """Mock the boto3 module for S3 testing."""
    mock_module = MagicMock()
    mock_module.client.return_value = MagicMock()
    return mock_module


@pytest.fixture
def mock_s3_env():
    """Fixture to set and restore S3-related environment variables."""
    original_env = os.environ.copy()
    os.environ['AWS_ACCESS_KEY_ID'] = 'mock-aws-key'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'mock-aws-secret'
    os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'
    os.environ['S3_BUCKET_NAME'] = 'mock-bucket'

    yield

    # Reset environment to original state
    os.environ.clear()
    os.environ.update(original_env)


</document_content>
</document>
<document index="5">
<source>./tests/test_db_manager.py</source>
<document_content>
import os
import tempfile
from datetime import datetime
from unittest.mock import patch, MagicMock

import pytest

from sglawwatch_to_sqlite.db_manager import DatabaseManager
from sglawwatch_to_sqlite.storage import DB_FILENAME


@pytest.fixture
def temp_dir():
    """Create a temporary directory for testing."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        yield tmpdirname


@pytest.fixture
def db_manager(temp_dir):
    """Setup a test database manager and return it."""
    return DatabaseManager(temp_dir)


def test_database_manager_initialization(temp_dir):
    """Test that DatabaseManager initializes correctly."""
    db_manager = DatabaseManager(temp_dir)
    db = db_manager.get_database()

    # Check if all expected tables exist
    table_names = db.table_names()
    assert "schema_versions" in table_names
    assert "headlines" in table_names
    assert "metadata" in table_names

    # Check if schema_versions has expected schema
    schema_versions_schema = db["schema_versions"].columns_dict
    assert "table_name" in schema_versions_schema
    assert "version" in schema_versions_schema
    assert "updated_at" in schema_versions_schema

    # Check if headlines table has expected schema
    headlines_schema = db["headlines"].columns_dict
    assert "id" in headlines_schema
    assert "category" in headlines_schema
    assert "title" in headlines_schema
    assert "source_link" in headlines_schema
    assert "author" in headlines_schema
    assert "date" in headlines_schema
    assert "summary" in headlines_schema
    assert "text" in headlines_schema
    assert "imported_on" in headlines_schema

    # Check if metadata has expected schema
    metadata_schema = db["metadata"].columns_dict
    assert "key" in metadata_schema
    assert "value" in metadata_schema

    # Verify the database was created with the fixed name
    assert os.path.exists(os.path.join(temp_dir, DB_FILENAME))


def test_database_manager_save(db_manager):
    """Test DatabaseManager.save method."""
    # Mock the storage save method
    db_manager.storage.save = MagicMock(return_value="saved/path")

    # Call save
    result = db_manager.save()

    # Verify the mock was called correctly
    db_manager.storage.save.assert_called_once_with(db_manager.local_path)
    assert result == "saved/path"


def test_register_table_version(db_manager):
    """Test that _register_table_version adds entries to schema_versions."""
    db = db_manager.get_database()

    # First clear any existing entries for test table
    db["schema_versions"].delete_where("table_name = ?", ["test_table"])

    # Register a new version
    db_manager._register_table_version("test_table", 2)

    # Check if entry exists
    entry = db["schema_versions"].get("test_table")
    assert entry["table_name"] == "test_table"
    assert entry["version"] == 2

    # Parse date to ensure it's a valid ISO format
    try:
        datetime.fromisoformat(entry["updated_at"])
        valid_date = True
    except ValueError:
        valid_date = False

    assert valid_date


def test_get_last_updated_existing_key(db_manager):
    """Test get_last_updated when the key already exists."""
    db = db_manager.get_database()

    # Setup test data
    feed_type = "test_feed"
    test_timestamp = datetime.now().isoformat()
    db["metadata"].insert({"key": f"{feed_type}_last_updated", "value": test_timestamp})

    # Get the value
    result = db_manager.get_last_updated(feed_type)

    # Check result
    assert result == test_timestamp


def test_get_last_updated_new_key(db_manager):
    """Test get_last_updated when the key doesn't exist."""
    db = db_manager.get_database()

    # Setup test data - Make sure the key doesn't exist
    feed_type = "nonexistent_feed"
    db["metadata"].delete_where("key = ?", [f"{feed_type}_last_updated"])

    # Get the value
    result = db_manager.get_last_updated(feed_type)

    # Check result - Should be empty string and a new entry should be created
    assert result == ""

    # Verify new entry was created
    entry = db["metadata"].get(f"{feed_type}_last_updated")
    assert entry["key"] == f"{feed_type}_last_updated"
    assert entry["value"] == ""


def test_update_last_updated(db_manager):
    """Test update_last_updated function."""
    db = db_manager.get_database()

    # Test the function
    db_manager.update_last_updated("headlines", "2025-05-10T12:34:56")

    # Verify the value was updated
    entry = db["metadata"].get("headlines_last_updated")
    assert entry["value"] == "2025-05-10T12:34:56"


@patch("sglawwatch_to_sqlite.storage.S3Storage")
def test_database_manager_s3(mock_s3_storage, temp_dir):
    """Test DatabaseManager with S3 URI."""
    # Setup mock
    mock_instance = MagicMock()
    mock_instance.get_local_path.return_value = os.path.join(temp_dir, DB_FILENAME)
    mock_s3_storage.return_value = mock_instance

    # Create DatabaseManager with S3 URI
    db_manager = DatabaseManager("s3://test-bucket/path/")

    # Verify the mock was created and used
    mock_s3_storage.assert_called_once_with("s3://test-bucket/path/")
    mock_instance.get_local_path.assert_called_once()

    # Verify the database manager has the correct storage
    assert db_manager.storage == mock_instance
</document_content>
</document>
<document index="6">
<source>./tests/test_headlines.py</source>
<document_content>
import datetime
from unittest.mock import patch, MagicMock

import pytest

from sglawwatch_to_sqlite.resources.headlines import (
    convert_date_to_iso,
    process_entry,
    fetch_headlines
)


# Test convert_date_to_iso function
def test_convert_date_to_iso():
    # Test standard format
    assert convert_date_to_iso("08 May 2025 00:01:00") == "2025-05-08T00:01:00"

    # Test abbreviated month format
    assert convert_date_to_iso("08 May 2025 00:01:00") == "2025-05-08T00:01:00"

    mock_now = datetime.datetime(2025, 5, 8, 0, 1, 0)

    # Test error handling with mock datetime.now
    with patch('sglawwatch_to_sqlite.resources.headlines.datetime') as mock_datetime:
        mock_datetime.now.return_value = mock_now
        mock_datetime.strptime.side_effect = ValueError("Invalid date format")
        mock_datetime.fromisoformat = datetime.datetime.fromisoformat  # Keep this method working

        # Should return current date in ISO format when parsing fails
        assert convert_date_to_iso("invalid date") == "2025-05-08T00:01:00"



# Test process_entry function
@pytest.mark.asyncio
async def test_process_entry_new_entry():
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()
    mock_db = MagicMock()
    mock_db_manager.get_database.return_value = mock_db

    # Mock entry
    entry = {
        'published': '08 May 2025 00:01:00',
        'title': 'Test Article',
        'category': 'Legal',
        'link': 'https://example.com/article',
        'author': 'John Doe'
    }

    # Mock external functions
    with patch('sglawwatch_to_sqlite.resources.headlines.get_jina_reader_content',
               return_value="Article content"):
        with patch('sglawwatch_to_sqlite.resources.headlines.get_summary',
                   return_value="Article summary"):
            # Test with a last_updated timestamp before the entry date
            timestamp, is_new, entry_data = await process_entry(mock_db_manager, entry, "2025-05-07T00:00:00")

            # Verify
            assert timestamp.isoformat() == "2025-05-08T00:01:00"
            assert is_new is True
            assert entry_data is not None
            assert entry_data["title"] == "Test Article"

            # Verify database interaction
            mock_db_manager.get_database.assert_called_once()
            mock_db["headlines"].insert.assert_called_once()


@pytest.mark.asyncio
async def test_process_entry_existing_entry():
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()

    # Mock entry
    entry = {
        'published': '08 May 2025 00:01:00',
        'title': 'Test Article'
    }

    # Test with a last_updated timestamp after the entry date (should skip)
    timestamp, is_new, entry_data = await process_entry(mock_db_manager, entry, "2025-05-09T00:00:00")

    # Verify
    assert timestamp.isoformat() == "2025-05-08T00:01:00"
    assert is_new is False
    assert entry_data is None

    # Verify DB interaction was NOT performed
    mock_db_manager.get_database.assert_not_called()


# Test fetch_headlines with mock feed
@pytest.mark.asyncio
async def test_fetch_headlines(mock_feed):
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()
    mock_db_manager.get_last_updated.return_value = "2025-05-07T00:00:00"

    with patch('feedparser.parse', return_value=mock_feed):
        with patch('sglawwatch_to_sqlite.resources.headlines.process_entry',
                   side_effect=lambda db, entry, last_updated: (
                           datetime.datetime(2025, 5, 8, 0, 1), True, {"id": "123"})):
            # Test
            result = await fetch_headlines(mock_db_manager, "https://example.com/feed")

            # Verify
            assert len(result) > 0
            mock_db_manager.get_last_updated.assert_called_once_with("headlines")
            mock_db_manager.update_last_updated.assert_called_once()


# Test fetch_headlines with no entries
@pytest.mark.asyncio
async def test_fetch_headlines_no_entries():
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()

    # Create a feed with no entries
    empty_feed = MagicMock()
    empty_feed.bozo = False
    empty_feed.entries = []

    with patch('feedparser.parse', return_value=empty_feed):
        # Test
        result = await fetch_headlines(mock_db_manager, "https://example.com/feed")

        # Verify
        assert result == []


# Test fetch_headlines with feed error
@pytest.mark.asyncio
async def test_fetch_headlines_feed_error(mock_feed_error):
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()

    with patch('feedparser.parse', return_value=mock_feed_error):
        with patch('click.echo') as mock_echo:
            # Test
            result = await fetch_headlines(mock_db_manager, "https://example.com/feed")

            # Verify error is logged and empty result is returned
            mock_echo.assert_called_with(
                "No entries found in the feed.",
            )
            assert result == []


@pytest.mark.asyncio
async def test_fetch_headlines_skip_advertisements():
    """Test that headlines with titles starting with 'ADV:' are skipped."""
    # Create mock DatabaseManager
    mock_db_manager = MagicMock()
    mock_db_manager.get_last_updated.return_value = "2025-05-07T00:00:00"

    # Create a mock feed with regular and advertisement entries
    mock_feed = MagicMock()
    mock_feed.bozo = False
    mock_feed.entries = [
        {
            'title': 'Normal Article Title',
            'published': '08 May 2025 00:01:00',
            'link': 'https://example.com/normal'
        },
        {
            'title': 'ADV: Advertisement Article',
            'published': '08 May 2025 00:01:00',
            'link': 'https://example.com/adv'
        },
        {
            'title': 'Another Normal Article',
            'published': '08 May 2025 00:01:00',
            'link': 'https://example.com/another'
        }
    ]

    # Track which entries are processed
    processed_entries = []

    # Mock process_entry to track which entries get processed
    async def mock_process_entry(db, entry, last_updated):
        processed_entries.append(entry['title'])
        return datetime.datetime(2025, 5, 8, 0, 1), True, {"id": "123"}

    with patch('feedparser.parse', return_value=mock_feed):
        with patch('sglawwatch_to_sqlite.resources.headlines.process_entry',
                   side_effect=mock_process_entry):
            with patch('click.echo'):  # To silence output during tests
                # Test
                result = await fetch_headlines(mock_db_manager, "https://example.com/feed")

                # Verify
                assert len(processed_entries) == 2  # Only the non-ADV entries
                assert 'Normal Article Title' in processed_entries
                assert 'Another Normal Article' in processed_entries
                assert 'ADV: Advertisement Article' not in processed_entries

</document_content>
</document>
<document index="7">
<source>./tests/test_metadata_manager.py</source>
<document_content>
# tests/test_metadata_manager.py
import json
import os
import tempfile
from unittest.mock import patch, MagicMock

import click
import pytest

from sglawwatch_to_sqlite.metadata_manager import MetadataManager, METADATA_FILENAME, \
    DATABASE_NAME


@pytest.fixture
def temp_dir():
    """Create a temporary directory for testing."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        yield tmpdirname


@pytest.fixture
def sample_metadata():
    """Create a sample metadata.json content."""
    return {
        "title": "Test Datasette",
        "description": "Test description",
        "databases": {
            "other_db": {
                "title": "Other Database"
            }
        }
    }


@pytest.fixture
def sample_project_metadata():
    """Create a sample project metadata content."""
    return {
        "title": "Singapore Law Watch Headlines",
        "description": "A database of legal news headlines",
        "tables": {
            "headlines": {
                "title": "Legal Headlines",
                "sortable_columns": ["date"]
            }
        }
    }


@pytest.fixture
def metadata_file(temp_dir, sample_metadata):
    """Create a temporary metadata.json file."""
    metadata_path = os.path.join(temp_dir, METADATA_FILENAME)
    with open(metadata_path, 'w') as f:
        json.dump(sample_metadata, f)
    return metadata_path


@pytest.fixture
def mock_project_metadata(sample_project_metadata):
    """Mock the project_metadata.json file."""
    with patch('sglawwatch_to_sqlite.metadata_manager.pkg_resources.read_text',
               return_value=json.dumps(sample_project_metadata)):
        return sample_project_metadata


def test_metadata_manager_initialization(temp_dir, metadata_file, mock_project_metadata):
    """Test that MetadataManager initializes correctly."""
    with patch('os.path.dirname', return_value=os.path.dirname(metadata_file)):
        manager = MetadataManager(temp_dir)

        # Verify metadata was loaded correctly
        assert "title" in manager.metadata
        assert "databases" in manager.metadata
        assert "other_db" in manager.metadata["databases"]

        # Verify project metadata was loaded
        assert manager.project_metadata["title"] == "Singapore Law Watch Headlines"


def test_metadata_manager_missing_file(temp_dir):
    """Test behavior when metadata.json doesn't exist."""
    with pytest.raises(click.exceptions.Abort):
        MetadataManager(temp_dir)


def test_metadata_manager_invalid_json(temp_dir):
    """Test behavior when metadata.json contains invalid JSON."""
    # Create an invalid JSON file
    metadata_path = os.path.join(temp_dir, METADATA_FILENAME)
    with open(metadata_path, 'w') as f:
        f.write("{invalid json")

    # Mock project_metadata.json to exist
    with patch('os.path.exists', return_value=True):
        with patch('builtins.open') as mock_open:
            def side_effect(path, *args, **kwargs):
                if path == metadata_path:
                    # Use the real file for metadata.json
                    return open.__enter__(path, *args, **kwargs)
                else:
                    # Mock for project_metadata.json
                    mock = MagicMock()
                    mock.__enter__.return_value.read.return_value = "{}"
                    return mock

            mock_open.side_effect = side_effect

            # Initialization should raise Abort
            with pytest.raises(click.exceptions.Abort):
                MetadataManager(temp_dir)


def test_metadata_manager_update_no_changes(temp_dir, metadata_file, mock_project_metadata):
    """Test update when no changes are needed."""
    with patch('os.path.dirname') as mock_dirname:
        mock_dirname.return_value = temp_dir

        # Setup existing metadata to already include our project metadata
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)

        metadata["databases"][DATABASE_NAME] = mock_project_metadata

        with open(metadata_file, 'w') as f:
            json.dump(metadata, f)

        # Initialize manager and try to update
        manager = MetadataManager(temp_dir)
        changes_made, message = manager.update_metadata()

        assert not changes_made
        assert "already up to date" in message


def test_metadata_manager_update_with_changes(temp_dir, metadata_file, mock_project_metadata):
    """Test update when changes are needed."""
    with patch('os.path.dirname') as mock_dirname:
        mock_dirname.return_value = temp_dir

        # Initialize manager
        manager = MetadataManager(temp_dir)

        # Run update
        changes_made, message = manager.update_metadata()

        # Verify changes
        assert changes_made
        assert "updated" in message

        # Check that the file was updated
        with open(metadata_file, 'r') as f:
            updated_metadata = json.load(f)

        assert DATABASE_NAME in updated_metadata["databases"]
        assert updated_metadata["databases"][DATABASE_NAME]["title"] == mock_project_metadata["title"]


def test_metadata_manager_update_dry_run(temp_dir, metadata_file, mock_project_metadata):
    """Test update with dry run option."""
    with patch('os.path.dirname') as mock_dirname:
        mock_dirname.return_value = temp_dir

        # Initialize manager
        manager = MetadataManager(temp_dir)

        # Run update with dry run
        changes_made, message = manager.update_metadata(dry_run=True)

        # Verify changes would be made but weren't
        assert changes_made
        assert "Changes would be made" in message

        # Check that the file wasn't actually updated
        with open(metadata_file, 'r') as f:
            updated_metadata = json.load(f)

        assert DATABASE_NAME not in updated_metadata["databases"]


def test_metadata_manager_create_new_database_entry(temp_dir, metadata_file, mock_project_metadata):
    """Test update when database entry doesn't exist yet."""
    with patch('os.path.dirname') as mock_dirname:
        mock_dirname.return_value = temp_dir

        # Initialize manager
        manager = MetadataManager(temp_dir)

        # Run update
        changes_made, message = manager.update_metadata()

        # Verify changes
        assert changes_made
        assert "updated" in message

        # Check that the file was updated with new database entry
        with open(metadata_file, 'r') as f:
            updated_metadata = json.load(f)

        assert DATABASE_NAME in updated_metadata["databases"]
        assert updated_metadata["databases"][DATABASE_NAME] == mock_project_metadata

</document_content>
</document>
<document index="8">
<source>./tests/test_sglawwatch_to_sqlite.py</source>
<document_content>
import os
from unittest.mock import patch, MagicMock

from click.testing import CliRunner

from sglawwatch_to_sqlite.cli import cli
from sglawwatch_to_sqlite.storage import DB_FILENAME


def test_version():
    runner = CliRunner()
    with runner.isolated_filesystem():
        result = runner.invoke(cli, ["--version"])
        assert result.exit_code == 0
        assert result.output.startswith("cli, version ")


@patch("sglawwatch_to_sqlite.cli.DatabaseManager")
@patch("sglawwatch_to_sqlite.cli.asyncio.run")
def test_headlines_command_local(mock_run, mock_db_manager_class):
    """Test headlines command with local storage."""
    # Setup mocks
    mock_db_manager = MagicMock()
    mock_db_manager.save.return_value = "./data/sglawwatch.db"
    mock_db_manager_class.return_value = mock_db_manager

    # Run command
    runner = CliRunner()
    result = runner.invoke(cli, ["fetch", "headlines", "./data"])

    # Check result
    assert result.exit_code == 0

    # Verify mocks were called correctly
    mock_db_manager_class.assert_called_once_with("./data")
    mock_run.assert_called_once()
    mock_db_manager.save.assert_called_once()

    # Check output
    assert "Database saved to" in result.output


@patch("sglawwatch_to_sqlite.cli.DatabaseManager")
@patch("sglawwatch_to_sqlite.cli.asyncio.run")
def test_headlines_command_s3(mock_run, mock_db_manager_class):
    """Test headlines command with S3 storage."""
    # Setup mocks
    mock_db_manager = MagicMock()
    mock_db_manager.save.return_value = "s3://test-bucket/path/sglawwatch.db"
    mock_db_manager_class.return_value = mock_db_manager

    # Run command
    runner = CliRunner()
    result = runner.invoke(cli, ["fetch", "headlines", "s3://test-bucket/path/"])

    # Check result
    assert result.exit_code == 0

    # Verify mocks were called correctly
    mock_db_manager_class.assert_called_once_with("s3://test-bucket/path/")
    mock_run.assert_called_once()
    mock_db_manager.save.assert_called_once()

    # Check output
    assert "Database saved to s3://test-bucket/path/sglawwatch.db" in result.output


@patch("sglawwatch_to_sqlite.cli.DatabaseManager")
@patch("sglawwatch_to_sqlite.cli.asyncio.run")
def test_fetch_all_command(mock_run, mock_db_manager_class):
    """Test fetch all command."""
    # Setup mocks
    mock_db_manager = MagicMock()
    mock_db_manager.save.return_value = "./data/sglawwatch.db"
    mock_db_manager_class.return_value = mock_db_manager

    # Run command
    runner = CliRunner()
    result = runner.invoke(cli, ["fetch", "all", "./data", "--reset"])

    # Check result
    assert result.exit_code == 0

    # Verify mocks were called correctly
    mock_db_manager_class.assert_called_once_with("./data")
    mock_run.assert_called_once()

    # Check output
    assert "All feeds have been processed" in result.output


def test_headlines_command_integration():
    """Integration test for headlines command with actual filesystem."""
    runner = CliRunner()
    with runner.isolated_filesystem() as temp_dir:
        # Create a directory for the database
        data_dir = os.path.join(temp_dir, "data")
        os.makedirs(data_dir)

        # Mock the fetch_headlines function to avoid actual network calls
        with patch("sglawwatch_to_sqlite.resources.headlines.fetch_headlines") as mock_fetch:
            # Run the command
            result = runner.invoke(cli, ["fetch", "headlines", data_dir])

            # Check the result
            assert result.exit_code == 0

            # Verify the database file was created
            assert os.path.exists(os.path.join(data_dir, DB_FILENAME))

            # Verify fetch_headlines was called
            mock_fetch.assert_called_once()
</document_content>
</document>
<document index="9">
<source>./tests/test_storage.py</source>
<document_content>
import os
import tempfile
from unittest.mock import patch, MagicMock

import click
import pytest
from botocore.exceptions import ClientError

from sglawwatch_to_sqlite.storage import Storage, LocalStorage, S3Storage, DB_FILENAME


@pytest.fixture
def temp_dir():
    """Create a temporary directory for testing."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        yield tmpdirname


def test_storage_factory_local():
    """Test Storage.create factory method with local path."""
    storage = Storage.create("./data")
    assert isinstance(storage, LocalStorage)
    assert storage.directory == "./data"
    assert storage.path.endswith(DB_FILENAME)


def test_storage_factory_s3():
    """Test Storage.create factory method with S3 URI."""
    storage = Storage.create("s3://my-bucket/path/")
    assert isinstance(storage, S3Storage)
    assert storage.bucket == "my-bucket"
    assert storage.key == f"path/{DB_FILENAME}"


def test_local_storage_init():
    """Test LocalStorage initialization."""
    # With directory path
    storage = LocalStorage("./data")
    assert storage.directory == "./data"
    assert storage.path == f"./data/{DB_FILENAME}"

    # With file path
    storage = LocalStorage("./data/mydb.db")
    assert storage.directory == "./data"
    assert storage.path == f"./data/{DB_FILENAME}"

    # With current directory
    storage = LocalStorage(".")
    assert storage.directory == "."
    assert storage.path == f"./{DB_FILENAME}"


def test_local_storage_get_local_path(temp_dir):
    """Test LocalStorage.get_local_path with a temporary directory."""
    sub_dir = os.path.join(temp_dir, "data")
    storage = LocalStorage(sub_dir)

    # Directory shouldn't exist yet
    assert not os.path.exists(sub_dir)

    # get_local_path should create the directory
    path = storage.get_local_path()
    assert os.path.exists(sub_dir)
    assert path == os.path.join(sub_dir, DB_FILENAME)


def test_local_storage_save(temp_dir):
    """Test LocalStorage.save method."""
    # Create a source file
    source_path = os.path.join(temp_dir, "source.db")
    with open(source_path, "w") as f:
        f.write("test data")

    # Setup storage in a subdirectory
    dest_dir = os.path.join(temp_dir, "dest")
    storage = LocalStorage(dest_dir)

    # Save the file
    result = storage.save(source_path)

    # Check the result
    assert result == os.path.join(dest_dir, DB_FILENAME)
    assert os.path.exists(result)

    # Check the content was copied
    with open(result, "r") as f:
        assert f.read() == "test data"


def test_s3_storage_init():
    """Test S3Storage initialization with various URI formats."""
    # With bucket and path
    storage = S3Storage("s3://my-bucket/path/to/data/")
    assert storage.bucket == "my-bucket"
    assert storage.key == f"path/to/data/{DB_FILENAME}"

    # With bucket but no path
    storage = S3Storage("s3://my-bucket")
    assert storage.bucket == "my-bucket"
    assert storage.key == DB_FILENAME

    # With bucket and file
    storage = S3Storage("s3://my-bucket/path/to/data/custom.db")
    assert storage.bucket == "my-bucket"
    assert storage.key == "path/to/data/custom.db"


def test_s3_storage_from_env():
    """Test S3Storage initialization with bucket from environment."""
    # Set environment variable
    with patch.dict(os.environ, {"S3_BUCKET_NAME": "env-bucket"}):
        # Without bucket in URI
        storage = S3Storage("s3:///path/to/data/")
        assert storage.bucket == "env-bucket"
        assert storage.key == f"path/to/data/{DB_FILENAME}"


def test_s3_storage_missing_bucket():
    """Test S3Storage initialization with missing bucket."""
    # Clear environment variable
    with patch.dict(os.environ, {"S3_BUCKET_NAME": ""}):
        # Without bucket in URI
        with pytest.raises(click.exceptions.Abort):
            with patch("click.echo") as mock_echo:
                S3Storage("s3:///path/to/data/")
                # This will be called but we won't get here because of the exception
                mock_echo.assert_called_with(
                    "Error: S3 bucket name must be specified either in the URI or via S3_BUCKET_NAME environment variable",
                    err=True
                )


@patch("boto3.client")
def test_s3_storage_get_local_path_new_file(mock_boto3_client):
    """Test S3Storage.get_local_path when the file doesn't exist in S3."""
    # Mock S3 client
    mock_s3 = MagicMock()
    mock_boto3_client.return_value = mock_s3

    # Setup mock to simulate file not found
    mock_error = ClientError(
        error_response={"Error": {"Code": "404"}},
        operation_name="download_file"
    )
    mock_s3.download_file.side_effect = mock_error

    # Test the method
    storage = S3Storage("s3://test-bucket/path/")
    local_path = storage.get_local_path()

    # Check the result
    assert local_path.endswith(".db")
    assert os.path.exists(local_path)

    # Verify the mock was called correctly
    mock_s3.download_file.assert_called_once_with(
        "test-bucket", f"path/{DB_FILENAME}", local_path
    )


@patch("boto3.client")
def test_s3_storage_get_local_path_existing_file(mock_boto3_client):
    """Test S3Storage.get_local_path when the file exists in S3."""
    # Mock S3 client
    mock_s3 = MagicMock()
    mock_boto3_client.return_value = mock_s3

    # Setup mock to write a test file when download_file is called
    def side_effect(bucket, key, filename):
        with open(filename, "w") as f:
            f.write("test data from s3")

    mock_s3.download_file.side_effect = side_effect

    # Test the method
    storage = S3Storage("s3://test-bucket/path/")
    local_path = storage.get_local_path()

    # Check the result
    assert local_path.endswith(".db")
    assert os.path.exists(local_path)

    # Check the content
    with open(local_path, "r") as f:
        assert f.read() == "test data from s3"

    # Verify the mock was called correctly
    mock_s3.download_file.assert_called_once_with(
        "test-bucket", f"path/{DB_FILENAME}", local_path
    )


@patch("boto3.client")
def test_s3_storage_save(mock_boto3_client):
    """Test S3Storage.save method."""
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(b"test data to upload")
        tmp_path = tmp.name

    try:
        # Mock S3 client
        mock_s3 = MagicMock()
        mock_boto3_client.return_value = mock_s3

        # Test the method
        storage = S3Storage("s3://test-bucket/path/")
        result = storage.save(tmp_path)

        # Check the result
        assert result == f"s3://test-bucket/path/{DB_FILENAME}"

        # Verify the mock was called correctly
        mock_s3.upload_file.assert_called_once_with(
            tmp_path, "test-bucket", f"path/{DB_FILENAME}"
        )
    finally:
        # Clean up
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)
</document_content>
</document>
<document index="10">
<source>./tests/test_tools.py</source>
<document_content>
import datetime
import hashlib
from unittest.mock import patch

import pytest

from sglawwatch_to_sqlite.tools import get_jina_reader_content, get_summary, get_hash_id


@pytest.mark.asyncio
async def test_get_jina_reader_content(mock_httpx_client, mock_env_vars):
    with patch('httpx.AsyncClient', return_value=mock_httpx_client):
        # Test
        content = await get_jina_reader_content("https://example.com/article")

        # Verify
        assert content == "<article>Sample article content</article>"
        # Verify correct URL and headers were used
        mock_httpx_client.__aenter__.return_value.get.assert_called_once()


@pytest.mark.asyncio
async def test_get_summary(mock_openai_client):
    with patch.dict('os.environ', {'OPENAI_API_KEY': 'fake-api-key'}):
        with patch('openai.AsyncOpenAI', return_value=mock_openai_client):
            # Test
            summary = await get_summary("This is a long article text that needs summarizing.")

            # Verify
            assert summary == "This is a concise legal summary."
            # Verify OpenAI was called with correct parameters
            mock_openai_client.responses.create.assert_called_once()


class TestGetHashId:
    def test_basic_functionality(self):
        """Test the function with basic string inputs."""
        result = get_hash_id(["2025-05-16", "Meeting Notes"])
        expected = hashlib.md5("2025-05-16|Meeting Notes".encode()).hexdigest()
        assert result == expected

    def test_single_element(self):
        """Test the function with a single element."""
        result = get_hash_id(["single_element"])
        expected = hashlib.md5("single_element".encode()).hexdigest()
        assert result == expected

    def test_custom_delimiter(self):
        """Test the function with a custom delimiter."""
        result = get_hash_id(["user", "login", "192.168.1.1"], delimiter=":")
        expected = hashlib.md5("user:login:192.168.1.1".encode()).hexdigest()
        assert result == expected

    def test_non_string_elements(self):
        """Test the function with non-string elements."""
        result = get_hash_id([123, True, 3.14])
        expected = hashlib.md5("123|True|3.14".encode()).hexdigest()
        assert result == expected

    def test_empty_list_raises_error(self):
        """Test that an empty list raises a ValueError."""
        with pytest.raises(ValueError, match="At least one element is required"):
            get_hash_id([])

    @pytest.mark.parametrize(
        "elements,delimiter,expected",
        [
            (
                ["2025-05-16", "Meeting Notes"],
                "|",
                hashlib.md5("2025-05-16|Meeting Notes".encode()).hexdigest(),
            ),
            (
                ["user", "login", "192.168.1.1"],
                ":",
                hashlib.md5("user:login:192.168.1.1".encode()).hexdigest(),
            ),
            (
                [123, "test"],
                "-",
                hashlib.md5("123-test".encode()).hexdigest(),
            ),
        ],
    )
    def test_parametrized_inputs(self, elements, delimiter, expected):
        """Test various input combinations."""
        result = get_hash_id(elements, delimiter)
        assert result == expected

    def test_consistency(self):
        """Test that the same input always produces the same output."""
        elements = ["2025-05-16", "Meeting Notes", "Confidential"]
        first_result = get_hash_id(elements)
        second_result = get_hash_id(elements)
        assert first_result == second_result
</document_content>
</document>
<document index="11">
<source>./sglawwatch_to_sqlite/__init__.py</source>
<document_content>
__version__ = "0.2.0"

</document_content>
</document>
<document index="12">
<source>./sglawwatch_to_sqlite/__main__.py</source>
<document_content>
from .cli import cli

if __name__ == "__main__":
    cli()

</document_content>
</document>
<document index="13">
<source>./sglawwatch_to_sqlite/cli.py</source>
<document_content>
import asyncio
import os

import click

from sglawwatch_to_sqlite.db_manager import DatabaseManager
from sglawwatch_to_sqlite.metadata_manager import MetadataManager
from sglawwatch_to_sqlite.storage import DB_FILENAME


@click.group()
@click.version_option()
def cli():
    """Track Singapore's legal developments by importing Singapore Law Watch's RSS feed into a searchable SQLite database"""


@cli.group(name="fetch")
def fetch():
    """Fetch entries from Singapore Law Watch RSS feeds into a SQLite database."""
    pass


@fetch.command(name="headlines")
@click.argument(
    "location",
    type=str,
    required=False,
    default=".",
)
@click.option(
    "--url",
    default="https://www.singaporelawwatch.sg/Portals/0/RSS/Headlines.xml",
    help="URL of the Singapore Law Watch Headlines RSS feed",
)
@click.option(
    "--all",
    is_flag=True,
    help="Fetch all entries regardless of last run state",
)
@click.option("--update-metadata", is_flag=True, help="Update Datasette project_metadata.json after fetching")
def headlines_command(location, url, all, update_metadata):
    """Fetch headline entries from Singapore Law Watch RSS feed.

    LOCATION can be a local directory or an S3 path (s3://bucket/path/).
    The database will always be named 'sglawwatch.db'.

    If LOCATION is not specified, the current directory is used.

    For S3 storage, you can also set the S3_BUCKET_NAME environment variable
    instead of including it in the path.
    """
    # Create a database manager
    db_manager = DatabaseManager(location)

    # Import here to avoid circular imports
    from sglawwatch_to_sqlite.resources.headlines import fetch_headlines

    # Run the fetch operation asynchronously
    asyncio.run(fetch_headlines(db_manager, url, all))

    # Save the database (this will upload to S3 if needed)
    saved_location = db_manager.save()

    if location.startswith('s3://'):
        click.echo(f"Database saved to {saved_location}")
    else:
        # For local storage, make the path more user-friendly
        rel_path = os.path.join(location, DB_FILENAME)
        if os.path.isabs(rel_path):
            click.echo(f"Database saved to {rel_path}")
        else:
            # Convert to relative path for better readability
            click.echo(f"Database saved to ./{rel_path}")

    if update_metadata:
        try:
            metadata_manager = MetadataManager(location)
            changes_made, message = metadata_manager.update_metadata()
            click.echo(message)
        except Exception as e:
            click.echo(f"Warning: Failed to update metadata: {e}", err=True)


# Add a command to fetch all feed types at once
@fetch.command(name="all")
@click.argument(
    "location",
    type=str,
    required=False,
    default=".",
)
@click.option(
    "--reset",
    is_flag=True,
    help="Reset and fetch all entries from scratch",
)
@click.option("--update-metadata", is_flag=True, help="Update Datasette project_metadata.json after fetching")
def fetch_all(location, reset, update_metadata):
    """Fetch all available feeds (headlines and judgments).

    LOCATION can be a local directory or an S3 path (s3://bucket/path/).
    The database will always be named 'sglawwatch.db'.

    If LOCATION is not specified, the current directory is used.

    For S3 storage, you can also set the S3_BUCKET_NAME environment variable
    instead of including it in the path.
    """
    click.echo("Fetching all Singapore Law Watch feeds...")

    ctx = click.get_current_context()

    # Fetch headlines
    ctx.invoke(headlines_command, location=location, all=reset, update_metadata=False)

    if update_metadata:
        try:
            metadata_manager = MetadataManager(location)
            changes_made, message = metadata_manager.update_metadata()
            click.echo(message)
        except Exception as e:
            click.echo(f"Warning: Failed to update metadata: {e}", err=True)

    click.echo("All feeds have been processed")


@cli.group(name="metadata")
def metadata():
    """Manage Datasette metadata for the Singapore Law Watch database."""
    pass


@metadata.command(name="update")
@click.argument("location", type=str, required=False, default=".")
@click.option("--dry-run", is_flag=True, help="Show changes without applying them")
def metadata_update(location, dry_run):
    """Update Datasette project_metadata.json with Singapore Law Watch database metadata.

    LOCATION can be a local directory or an S3 path (s3://bucket/path/).
    If LOCATION is not specified, the current directory is used.
    """
    try:
        metadata_manager = MetadataManager(location)
        changes_made, message = metadata_manager.update_metadata(dry_run)
        click.echo(message)
    except Exception as e:
        click.echo(f"Error updating metadata: {e}", err=True)
        raise click.Abort()

</document_content>
</document>
<document index="14">
<source>./sglawwatch_to_sqlite/db_manager.py</source>
<document_content>
from datetime import datetime

import click
import sqlite_utils

from sglawwatch_to_sqlite.storage import Storage

# Current table versions
TABLE_VERSIONS = {
    "headlines": 1,
    "metadata": 1
}


class DatabaseManager:
    """
    A class that manages both the database and its storage.
    """

    def __init__(self, database_uri):
        """
        Initialize a DatabaseManager with a database URI.

        Args:
            database_uri: Either a local file path or an S3 URI (s3://bucket/path)
        """
        try:
            # Create the appropriate storage
            self.storage = Storage.create(database_uri)

            # Get the local path (will download from S3 if needed)
            self.local_path = self.storage.get_local_path()

            # Connect to the database
            self.db = sqlite_utils.Database(self.local_path)

            # Set up tables if needed
            self._setup_tables()
        except Exception as e:
            click.echo(f"Error connecting to database at {database_uri}: {e}", err=True)
            raise click.Abort()

    def _setup_tables(self):
        """Set up the necessary tables in the database."""
        try:
            # Check/create schema_versions table first
            if "schema_versions" not in self.db.table_names():
                self.db["schema_versions"].create({
                    "table_name": str,
                    "version": int,
                    "updated_at": str
                }, pk="table_name")
                click.echo("Created schema version tracking table")

            # Create the headlines table if it doesn't exist
            if "headlines" not in self.db.table_names():
                self.db["headlines"].create({
                    "id": str,  # Unique identifier for each article
                    "category": str,  # The category of the news article
                    "title": str,  # The title of the article
                    "source_link": str,  # URL to the source article
                    "author": str,  # Author of the article
                    "date": str,  # Publication date in ISO format
                    "summary": str,  # Summary text
                    "text": str,  # Full text content
                    "imported_on": str  # When the article was imported
                }, pk="id")

                # Create indexes for common query patterns
                self.db["headlines"].create_index(["date"])
                self.db["headlines"].create_index(["author"])

                self.db["headlines"].enable_fts(["title", "summary"], create_triggers=True)

                self._register_table_version("headlines", TABLE_VERSIONS["headlines"])

            # Create the metadata table if it doesn't exist
            if "metadata" not in self.db.table_names():
                self.db["metadata"].create({
                    "key": str,
                    "value": str
                }, pk="key")
                self._register_table_version("metadata", TABLE_VERSIONS["metadata"])

        except Exception as e:
            click.echo(f"Error creating table: {e}", err=True)
            raise click.Abort()

    def _register_table_version(self, table_name, version):
        """Register a new table version in the schema_versions table"""
        self.db["schema_versions"].insert({
            "table_name": table_name,
            "version": version,
            "updated_at": datetime.now().isoformat()
        })
        click.echo(f"Registered {table_name} table with schema version {version}")

    def get_database(self):
        """Get the sqlite_utils Database object."""
        return self.db

    def save(self):
        """Save the database, handling S3 upload if needed."""
        return self.storage.save(self.local_path)

    def get_last_updated(self, feed_type):
        """Get the last updated timestamp for a specific feed type"""
        metadata_key = f"{feed_type}_last_updated"
        try:
            return self.db["metadata"].get(metadata_key)["value"]
        except sqlite_utils.db.NotFoundError:
            self.db["metadata"].insert({"key": metadata_key, "value": ""})
            return ""

    def update_last_updated(self, feed_type, timestamp):
        """Update the last updated timestamp for a specific feed type"""
        metadata_key = f"{feed_type}_last_updated"
        self.db["metadata"].upsert({"key": metadata_key, "value": timestamp}, pk="key")

</document_content>
</document>
<document index="15">
<source>./sglawwatch_to_sqlite/metadata_manager.py</source>
<document_content>
"""
Datasette Metadata Manager

This module manages Datasette metadata integration for the sglawwatch-to-sqlite tool.

How it works:
- Loads existing metadata.json from local or S3 storage
- Updates it with project-specific configuration from repository project_metadata.json
- Preserves other database configs in the same file
- Calculates hash to determine if updates are needed

CLI usage:
    # Dedicated update command
    sglawwatch-to-sqlite metadata update ./data [--dry-run]

    # With fetch commands
    sglawwatch-to-sqlite fetch headlines ./data --update-metadata
    sglawwatch-to-sqlite fetch all ./data --update-metadata

    # S3 storage
    sglawwatch-to-sqlite metadata update s3://bucket/path/ [--dry-run]

Customization:
- Edit repository project_metadata.json to change how database appears in Datasette
- Configure tables, columns, facets, and database-level metadata
- Run update command to apply changes

Requirements:
- project_metadata.json must exist in target location
- S3 storage requires proper read/write permissions
- Database name is always "sglawwatch" (without .db extension)

See: https://docs.datasette.io/en/stable/metadata.html for Datasette metadata options
"""

import json
import os
import importlib.resources as pkg_resources

import click

from sglawwatch_to_sqlite.storage import Storage
from sglawwatch_to_sqlite.tools import get_hash_id

DATABASE_NAME = "sglawwatch"


# Filename constants
METADATA_FILENAME = "metadata.json"


class MetadataManager:
    """
    Manages Datasette metadata.json, adding project-specific metadata.
    """

    def __init__(self, database_uri):
        """
        Initialize a MetadataManager with a database URI.

        Args:
            database_uri: Either a local file path or an S3 URI (s3://bucket/path)
        """
        try:
            # Create the appropriate storage
            self.storage = Storage.create(database_uri)

            # Get the local path for metadata.json
            try:
                self.local_path = self.storage.get_local_path(filename=METADATA_FILENAME)
                # Load existing metadata if it exists
                if os.path.exists(self.local_path):
                    with open(self.local_path, 'r') as f:
                        self.metadata = json.load(f)
                else:
                    raise FileNotFoundError(f"No existing {METADATA_FILENAME} found")
            except FileNotFoundError as e:
                click.echo(f"Error: {e}. Cannot update non-existent metadata file.", err=True)
                raise click.Abort()

            # Load project metadata template (now using project_metadata.json)
            project_data = pkg_resources.read_text(
                'sglawwatch_to_sqlite',
                'project_metadata.json'
            )
            self.project_metadata = json.loads(project_data)

        except json.JSONDecodeError as e:
            click.echo(f"Error parsing JSON: {e}", err=True)
            raise click.Abort()
        except Exception as e:
            click.echo(f"Error initializing metadata manager at {database_uri}: {e}", err=True)
            raise click.Abort()

    def update_metadata(self, dry_run=False):
        """
        Update Datasette metadata with project metadata.

        Args:
            dry_run: If True, don't save changes, just preview them

        Returns:
            A tuple (bool, str) indicating if changes were made and a message
        """
        # Check if database entry exists in the metadata
        db_name = DATABASE_NAME  # DB filename without extension

        # Initialize database metadata if it doesn't exist
        if "databases" not in self.metadata:
            self.metadata["databases"] = {}

        if db_name not in self.metadata["databases"]:
            self.metadata["databases"][db_name] = {}
            changes_needed = True
        else:
            # Get current database metadata
            current_db_metadata = self.metadata["databases"][db_name]

            # Direct dictionary comparison instead of hash comparison
            # This is more reliable than hashing for detecting changes
            changes_needed = current_db_metadata != self.project_metadata

        if not changes_needed:
            message = "No changes needed - metadata is already up to date"
            return False, message

        # Update the metadata
        self.metadata["databases"][db_name] = self.project_metadata

        if dry_run:
            message = f"Changes would be made to {METADATA_FILENAME} (dry run):\n"
            message += json.dumps(self.metadata, indent=2)
            return True, message

        # Save the updated metadata
        with open(self.local_path, 'w') as f:
            json.dump(self.metadata, f, indent=2)

        # Save to storage location
        saved_location = self.storage.save(self.local_path, filename=METADATA_FILENAME)

        message = f"Metadata updated and saved to {saved_location}"
        return True, message

</document_content>
</document>
<document index="16">
<source>./sglawwatch_to_sqlite/project_metadata.json</source>
<document_content>
{
  "title": "Singapore Law Watch Headlines",
  "description": "A database of legal news headlines from Singapore Law Watch",
  "license": "Apache License 2.0",
  "license_url": "https://github.com/houfu/sglawwatch-to-sqlite/blob/master/LICENSE",
  "source": "Singapore Law Watch",
  "source_url": "https://www.singaporelawwatch.sg/",
  "about": "This database contains legal news headlines imported from Singapore Law Watch's RSS feed.",
  "about_url": "https://github.com/houfu/sglawwatch-to-sqlite",
  "tables": {
    "headlines": {
      "title": "Legal Headlines",
      "description": "Headlines from Singapore Law Watch's RSS feed",
      "sortable_columns": [
        "date",
        "author"
      ],
      "facets": [
        "category",
        "author",
        "date"
      ],
      "columns": {
        "id": {
          "title": "ID",
          "description": "Unique identifier for each headline"
        },
        "category": {
          "title": "Category",
          "description": "The category of the news article"
        },
        "title": {
          "title": "Title",
          "description": "The headline title"
        },
        "source_link": {
          "title": "Source",
          "description": "URL to the original article"
        },
        "author": {
          "title": "Author",
          "description": "The author or publication"
        },
        "date": {
          "title": "Date",
          "description": "Publication date in ISO format"
        },
        "summary": {
          "title": "Summary",
          "description": "AI-generated summary of the article"
        },
        "text": {
          "title": "Content",
          "description": "Full text content of the article"
        },
        "imported_on": {
          "title": "Imported On",
          "description": "When the article was imported into the database"
        }
      }
    }
  }
}
</document_content>
</document>
<document index="17">
<source>./sglawwatch_to_sqlite/storage.py</source>
<document_content>
import os
import tempfile
from urllib.parse import urlparse

import click

from sglawwatch_to_sqlite.tools import verify_boto3

# Fixed database filename
DB_FILENAME = "sglawwatch.db"


class Storage:
    """
    Abstract base class for database storage.
    """

    def get_local_path(self, filename=DB_FILENAME):
        """Get the local path to the file"""
        raise NotImplementedError()

    def save(self, local_path=None, filename=DB_FILENAME):
        """Save the file"""
        raise NotImplementedError()

    @staticmethod
    def create(location):
        """
        Factory method to create the appropriate storage object.

        Args:
            location: Either a local directory path or an S3 URI (s3://bucket/path/)
                      If no location is specified, the current directory is used.
        """
        if not location:
            # Default to current directory
            return LocalStorage(".")

        if location.startswith('s3://'):
            return S3Storage(location)
        else:
            return LocalStorage(location)


class LocalStorage(Storage):
    """
    Local filesystem storage for the database.
    """

    def __init__(self, directory):
        # Ensure the directory doesn't have a filename at the end
        if os.path.isfile(directory) or directory.endswith('.db'):
            directory = os.path.dirname(directory) or "."

        self.directory = directory
        self.path = os.path.join(directory, DB_FILENAME)

    def get_local_path(self, filename=DB_FILENAME):
        # Ensure the directory exists
        if self.directory and not os.path.exists(self.directory):
            os.makedirs(self.directory, exist_ok=True)
        return os.path.join(self.directory, filename)

    def save(self, local_path=None, filename=DB_FILENAME):
        """
        Save a file to the storage location.

        Args:
            local_path: Path to the local file to save.
                       If None, assumes the file is already at self.path.
            filename: Name of the file to save.

        Returns:
            The final path where the file was saved.
        """
        target_path = os.path.join(self.directory, filename)

        # For local storage, nothing needs to be done if the path is the same
        if local_path and local_path != target_path:
            import shutil

            # Make sure the target directory exists
            if not os.path.exists(self.directory):
                os.makedirs(self.directory)

            shutil.copy2(local_path, target_path)
        return target_path


class S3Storage(Storage):
    """
    S3 storage for the database.
    """

    def __init__(self, s3_uri):
        self.s3_uri = s3_uri

        # Parse the S3 URI
        parsed = urlparse(s3_uri)

        # Check if we have a bucket name in the URI
        if parsed.netloc:
            self.bucket = parsed.netloc
        else:
            # Try to get bucket name from environment variable
            self.bucket = os.environ.get('S3_BUCKET_NAME')
            if not self.bucket:
                click.echo(
                    "Error: S3 bucket name must be specified either in the URI or via S3_BUCKET_NAME environment variable",
                    err=True)
                raise click.Abort()

        # Parse the key (path in the bucket)
        self.key = parsed.path.lstrip('/')

        # If the key doesn't end with a filename, append the fixed DB filename
        if not self.key or self.key.endswith('/'):
            self.key = f"{self.key}{DB_FILENAME}"
        elif not os.path.basename(self.key) or not os.path.splitext(self.key)[1]:
            # It doesn't have a file extension, assume it's a directory
            self.key = f"{self.key}/{DB_FILENAME}"

        # Get endpoint URL from environment variable if available
        self.endpoint_url = os.environ.get('S3_ENDPOINT_URL')
        self.region_name = os.environ.get('AWS_DEFAULT_REGION', 'default')

        self._temp_file = None
        self._temp_files = {}

    def _get_s3_client(self):
        """Get an S3 client with proper configuration."""
        import boto3

        # Create boto3 client with custom endpoint if provided
        client_kwargs = {}
        if self.endpoint_url:
            client_kwargs['endpoint_url'] = self.endpoint_url
            client_kwargs['region_name'] = self.region_name

        return boto3.client('s3', **client_kwargs)

    def _get_full_key(self, filename):
        """Get the full S3 key for a filename."""
        if not self.key or self.key.endswith('/'):
            return f"{self.key}{filename}"
        else:
            # If key already has a filename, use the directory
            base_dir = os.path.dirname(self.key)
            if base_dir:
                return f"{base_dir}/{filename}"
            else:
                return filename

    def get_local_path(self, filename=DB_FILENAME):
        verify_boto3()

        # Create a temporary file
        temp_fd, temp_path = tempfile.mkstemp(suffix=os.path.splitext(filename)[1])
        os.close(temp_fd)
        self._temp_files[filename] = temp_path

        # Download the file from S3 if it exists
        try:
            from botocore.exceptions import ClientError

            s3_client = self._get_s3_client()
            full_key = self._get_full_key(filename)

            try:
                click.echo(f"Downloading {filename} from s3://{self.bucket}/{full_key}")
                s3_client.download_file(self.bucket, full_key, temp_path)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    if filename == DB_FILENAME:
                        click.echo(
                            f"No existing database found at s3://{self.bucket}/{full_key}. A new one will be created.")
                    else:
                        # For non-database files, raise a FileNotFoundError
                        raise FileNotFoundError(f"File {filename} not found at s3://{self.bucket}/{full_key}")
                else:
                    click.echo(f"Error downloading from S3: {e}", err=True)
                    raise click.Abort()
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise  # Re-raise FileNotFoundError for non-DB files
            click.echo(f"Error accessing S3: {e}", err=True)
            raise click.Abort()

        return temp_path

    def save(self, local_path=None, filename=DB_FILENAME):
        verify_boto3()

        if not local_path:
            local_path = self._temp_files.get(filename)

        if not local_path or not os.path.exists(local_path):
            click.echo(f"Error: Local file not found: {local_path}", err=True)
            raise click.Abort()

        try:
            s3_client = self._get_s3_client()
            full_key = self._get_full_key(filename)

            click.echo(f"Uploading {filename} to s3://{self.bucket}/{full_key}")
            s3_client.upload_file(local_path, self.bucket, full_key)
            click.echo(f"{filename} successfully uploaded to S3")
            return f"s3://{self.bucket}/{full_key}"
        except Exception as e:
            click.echo(f"Error uploading to S3: {e}", err=True)
            raise click.Abort()
        finally:
            # Clean up the temporary file
            if filename in self._temp_files and os.path.exists(self._temp_files[filename]):
                os.unlink(self._temp_files[filename])
                del self._temp_files[filename]

</document_content>
</document>
<document index="18">
<source>./sglawwatch_to_sqlite/tools.py</source>
<document_content>
import os

import click
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

SYSTEM_PROMPT_TEXT = "As an AI expert in legal affairs, your task is to provide concise, yet comprehensive " \
                     "summaries of legal news articles for time-constrained attorneys. These summaries " \
                     "should highlight the critical legal aspects, relevant precedents, and implications of " \
                     "the issues discussed in the articles.\n\nDespite their complexity, the summaries " \
                     "should be accessible and digestible, written in an engaging and conversational style. " \
                     "Accuracy and attention to detail are essential, as the readers will be legal " \
                     "professionals who may use these summaries to inform their practice.\n\n" \
                     "### Instructions: \n1. Begin the summary with a brief introduction of the topic of " \
                     "the article.\n2. Outline the main legal aspects, implications, and precedents " \
                     "highlighted in the article. \n3. End the summary with a succinct conclusion or " \
                     "takeaway.\n\nThe summaries should not be longer than 100 words, but ensure they " \
                     "efficiently deliver the key legal insights, making them beneficial for quick " \
                     "comprehension. The end goal is to help the lawyers understand the crux of the " \
                     "articles without having to read them in their entirety."


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=1, max=10))
async def get_jina_reader_content(link: str) -> str:
    """Fetch content from the Jina reader link."""
    jina_token = os.environ.get('JINA_API_TOKEN')
    if not jina_token:
        click.echo("JINA_API_TOKEN environment variable not set", err=True)
        return ""
    jina_link = f"https://r.jina.ai/{link}"
    headers = {
        "Authorization": f"Bearer {jina_token}",
        "X-Retain-Images": "none",
        "X-Target-Selector": "article"
    }
    try:
        async with httpx.AsyncClient(timeout=90) as client:
            r = await client.get(jina_link, headers=headers)
        return r.text
    except httpx.RequestError as e:
        click.echo(f"Error fetching content from Jina reader: {e}", err=True)
        return ""


async def get_summary(text: str) -> str:
    """Generate a summary of the article text using OpenAI."""
    if not os.environ.get('OPENAI_API_KEY'):
        click.echo("OPENAI_API_KEY environment variable not set", err=True)
        return ""
    from openai import AsyncOpenAI
    client = AsyncOpenAI(max_retries=3, timeout=60)
    try:
        response = await client.responses.create(
            model="gpt-4.1-mini",
            input=[
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "input_text",
                            "text": SYSTEM_PROMPT_TEXT
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "input_text",
                            "text": f"Here is an article to summarise:\n {text}"
                        }
                    ]
                }
            ],
            text={
                "format": {
                    "type": "text"
                }
            },
            temperature=0.42,
            max_output_tokens=2048,
            top_p=1,
            store=False
        )
        return response.output_text
    except Exception as e:
        click.echo(f"Error generating summary from OpenAI: {e}", err=True)
        return ""


def get_hash_id(elements: list[str], delimiter: str = "|") -> str:
    """Generate a hash ID from a list of strings.

    Args:
        elements: List of strings to be hashed.
        delimiter: String used to join elements (default: "|").

    Returns:
        A hexadecimal MD5 hash of the joined elements.

    Examples:
        >>> get_hash_id(["2025-05-16", "Meeting Notes"])
        '1a2b3c4d5e6f7g8h9i0j'

        >>> get_hash_id(["user123", "login", "192.168.1.1"], delimiter=":")
        '7h8i9j0k1l2m3n4o5p6q'
    """
    import hashlib

    if not elements:
        raise ValueError("At least one element is required")

    joined_string = delimiter.join(str(element) for element in elements)
    return hashlib.md5(joined_string.encode()).hexdigest()


def verify_boto3():
    """Import boto3 and check if it's available."""
    try:
        import boto3  # noqa: F401
        return True
    except ImportError:
        click.echo("boto3 is required for S3 storage. Install it with 'uv install boto3'.", err=True)
        raise click.Abort()

</document_content>
</document>
<document index="19">
<source>./sglawwatch_to_sqlite/resources/__init__.py</source>
<document_content>

</document_content>
</document>
<document index="20">
<source>./sglawwatch_to_sqlite/resources/headlines.py</source>
<document_content>
import asyncio
from datetime import datetime
from typing import Tuple, Dict

import click
import feedparser

from sglawwatch_to_sqlite.db_manager import DatabaseManager
from sglawwatch_to_sqlite.tools import get_jina_reader_content, get_summary, get_hash_id


def convert_date_to_iso(date_str: str) -> str:
    """Convert date string like '08 May 2025 00:01:00' to ISO format."""
    try:
        parsed_date = datetime.strptime(date_str, '%d %B %Y %H:%M:%S')
        return parsed_date.isoformat()  # Returns '2025-05-08T00:01:00'
    except ValueError:
        # Handle potential parsing errors
        try:
            # Try alternative format with abbreviated month name
            parsed_date = datetime.strptime(date_str, '%d %b %Y %H:%M:%S')
            return parsed_date.isoformat()
        except ValueError:
            # If all parsing attempts fail, return original or a default
            return datetime.now().isoformat()


async def process_entry(db_manager: DatabaseManager, entry: Dict, last_updated: str) -> Tuple[datetime, bool, Dict]:
    """Process a single feed entry."""
    entry_date = datetime.fromisoformat(convert_date_to_iso(entry['published']))
    last_updated_date = datetime.fromisoformat(last_updated) if last_updated else None

    # Check if the entry is newer than the last updated date
    is_new_entry = True
    if last_updated_date:
        is_new_entry = entry_date > last_updated_date

    # Prepare the entry data
    entry_data = {
        "id": get_hash_id([entry_date.isoformat(), entry['title']]),
        "category": entry.get("category", ""),
        "title": entry.get("title", ""),
        "source_link": entry.get("link", ""),
        "author": entry.get("author", ""),
        "date": entry_date.isoformat(),
        "imported_on": datetime.now().isoformat()
    }

    # Echo information about the entry being processed
    click.echo(f"Processing: {entry_data['title']} from {entry_data['date']}")

    if is_new_entry:
        click.echo(f"  â†’ Fetching content for: {entry_data['title']}")
        entry_data["text"] = await get_jina_reader_content(entry_data["source_link"])

        click.echo(f"  â†’ Generating summary for: {entry_data['title']}")
        entry_data["summary"] = await get_summary(entry_data["text"])

        # Get the database and insert the new entry
        db = db_manager.get_database()
        db["headlines"].insert(entry_data, pk="id")
        click.echo(f"  âœ“ Added to database: {entry_data['title']}")
    else:
        click.echo(f"  â†’ Skipping (already processed): {entry_data['title']}")

    return entry_date, is_new_entry, entry_data if is_new_entry else None


async def fetch_headlines(db_manager: DatabaseManager, url: str, all_entries=False, max_age_limit=60) -> list:
    """Fetch headline entries from Singapore Law Watch RSS feed."""
    click.echo(f"Fetching headlines from {url}")

    # Get the last updated timestamp
    last_updated = None if all_entries else db_manager.get_last_updated("headlines")

    # Parse the RSS feed
    feed = feedparser.parse(url)

    if feed.bozo:
        click.echo(f"Warning: RSS feed parsing error - {feed.bozo_exception}", err=True)

    if not feed.entries:
        click.echo("No entries found in the feed.")
        return []

    # Track the most recent entry timestamp
    most_recent_timestamp: None | datetime = None
    new_entries_count = 0
    new_entries = []
    skipped_adv_count = 0
    skipped_old_count = 0
    current_date = datetime.now()

    tasks = []
    for entry in feed.entries:
        # Skip entries with titles starting with "ADV"
        if entry.get('title', '').startswith('ADV:'):
            skipped_adv_count += 1
            click.echo(f"Skipping advertisement: {entry.get('title', '')}")
            continue

        # Skip entries older than max_age_days
        entry_date = datetime.fromisoformat(convert_date_to_iso(entry.get('published', '')))
        days_old = (current_date - entry_date).days
        if days_old > max_age_limit:
            skipped_old_count += 1
            click.echo(f"Skipping old headline ({days_old} days): {entry.get('title', '')}")
            continue

        task = asyncio.create_task(process_entry(db_manager, entry, last_updated))
        tasks.append(task)

    # Wait for all tasks to complete
    results = await asyncio.gather(*tasks)

    # Process results
    for timestamp, is_new, entry_data in results:
        if is_new:
            new_entries_count += 1
            if entry_data:
                new_entries.append(entry_data)

        if not most_recent_timestamp or timestamp > most_recent_timestamp:
            most_recent_timestamp = timestamp

    if most_recent_timestamp:
        db_manager.update_last_updated("headlines", datetime.isoformat(most_recent_timestamp))

    click.echo(f"Added {new_entries_count} new headlines")
    if skipped_adv_count > 0:
        click.echo(f"Skipped {skipped_adv_count} advertisements")
    if skipped_old_count > 0:
        click.echo(f"Skipped {skipped_old_count} headlines older than {max_age_limit} days")

    return new_entries
</document_content>
</document>
</documents>
